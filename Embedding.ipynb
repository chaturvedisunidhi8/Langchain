{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d8348d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###store all api key in env (environment variable)\n",
    "###install pythondotenv---helps to call all your environment variable in your coding environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a26545",
   "metadata": {},
   "source": [
    "Embedding----converting text into vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca72dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "\n",
    "## bcz our notebook  isnt in our environment thats why i load api here\n",
    "#from dotenv import load_dotenv\n",
    "#load_dotenv()##load all environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c33e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "%pip install langchain-openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74afa934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x0000027317C7E990>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x0000027317C7F110>, model='text-embedding-3-large', dimensions=None, deployment='text-embedding-ada-002', openai_api_version=None, openai_api_base=None, openai_api_type=None, openai_proxy=None, embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings=OpenAIEmbeddings(model=\"text-embedding-3-large\")##this model is avail.at documentation in openai platform\n",
    "embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c7791d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"this is a lecture of openai\"\n",
    "query_result=embeddings.embed_query(text)\n",
    "query_result\n",
    "\n",
    "## quota exceede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c452f602",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2b32e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d69f95d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OpenAIEmbeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m##to set own dimensions\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m embeddings_1024=\u001b[43mOpenAIEmbeddings\u001b[49m(model=\u001b[33m\"\u001b[39m\u001b[33mtext-embedding-3-large\u001b[39m\u001b[33m\"\u001b[39m,dimensions=\u001b[32m1024\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'OpenAIEmbeddings' is not defined"
     ]
    }
   ],
   "source": [
    "##to set own dimensions\n",
    "embeddings_1024=OpenAIEmbeddings(model=\"text-embedding-3-large\",dimensions=1024)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff94316e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x000002731856C8A0>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x000002731856D5B0>, model='text-embedding-3-large', dimensions=1024, deployment='text-embedding-ada-002', openai_api_version=None, openai_api_base=None, openai_api_type=None, openai_proxy=None, embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed5a43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"this is a lecture of openai\"\n",
    "query_result=embeddings_1024.embed_query(text)\n",
    "query_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2662304",
   "metadata": {},
   "outputs": [],
   "source": [
    "##perform same thing speech.txt\n",
    "## step-1-- load speech .text using text loader\n",
    "## step-2-- convert into chunks using Recursivecharactersplitter\n",
    "## step-3-- convert text into vectors then store it in vectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa87567e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.text.TextLoader at 0x2361f9c6ba0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##step--1\n",
    "from langchain_community.document_loaders.text import TextLoader\n",
    "##from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(\"speech.txt\")   # create loader\n",
    "loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b00c331c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content='Good morning everyone,\\n\\nIt is a pleasure to be here today to talk about the power of technology in shaping our future. \\nFrom artificial intelligence to renewable energy, we are living in an era of rapid innovation. \\n\\nBut with great power comes great responsibility. \\nWe must ensure that technology is used not only to make our lives easier, but also to create a fairer and more sustainable world. \\n\\nThank you.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##step---2\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50)##chunk_size means each chunk has maximize size of 500\n",
    "final_document_splitter=text_splitter.split_documents(doc)## for both txt file and pdf\n",
    "final_document_splitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ee65f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##step--3 embedding and vector database\n",
    "from langchain_community.vectorstores import Chroma\n",
    "##if you have installed chroma  then directly use this\n",
    "##from langchain_chroma import chroma\n",
    "\n",
    "db=Chroma.from_documents(final_document_splitter,embeddings_1024) ##install chroma\n",
    "db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc78332",
   "metadata": {},
   "outputs": [],
   "source": [
    "##how to query vectorDB\n",
    "query=\"\"\"\"\n",
    "It is a pleasure to be here today to talk about the power of technology in shaping our future. \n",
    "From artificial intelligence to renewable energy, we are living in an era of rapid innovation. \n",
    "\n",
    "But with great power comes great responsibility. \n",
    "We must ensure that technology is used not only to make our lives easier, but also to create a fairer and more sustainable world. \n",
    "\"\"\"\n",
    "##retrive the result from query\n",
    "retrieved_result=db.similarity_search(query)\n",
    "retrieved_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba45ed90",
   "metadata": {},
   "outputs": [],
   "source": [
    "##save to the disk\n",
    "\n",
    "##db=Chroma.from_documents(final_document_splitter,embeddings_1024,persist_directory=\"./chroma_db\") here we give directory location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75625ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##load from the disk\n",
    "\n",
    "##db2=Chroma.from_documents(persist_directory=\"./chroma_db\",embedding_function=embedding)\n",
    "# docs=db2.similarity_search(query)\n",
    "#docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef9e72a",
   "metadata": {},
   "source": [
    "You can also use some other open source model like ollama instead of Openai\n",
    "oolama--kind of platform to use open source model such as llama3,mistral,gemma etc......."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4403ca",
   "metadata": {},
   "source": [
    "step 1 downlaod ollama from from its website\n",
    "than  whatever the model you want  to use (check from ollama webs.) choose model and paste it in cmd\n",
    "----ollama run gemma:2b( gemma:2b this is model name)---this helps to install in downlaod machine(first time)\n",
    "llarly.--- you can use any model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cd970d",
   "metadata": {},
   "source": [
    "RAG (Retrieval-Augmented Generation) & Vector Database\n",
    "\n",
    "A vector database is used to store embeddings (numerical vector representations of text, images, etc.) and efficiently retrieve semantically similar items using similarity search.\n",
    "\n",
    "RAG (Retrieval-Augmented Generation) is a technique that combines:\n",
    "\n",
    "1. Retrieval → fetching relevant information from a knowledge source (like a vector database).\n",
    "\n",
    "\n",
    "2. Generation → passing the retrieved information into a language model (LLM) so it can generate grounded, context-aware responses.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Role of RAG in Vector Databases\n",
    "\n",
    "Bridge between LLMs and knowledge bases\n",
    "RAG uses the vector database as its retrieval engine. Instead of relying only on the LLM’s fixed knowledge, it fetches up-to-date, domain-specific, or proprietary data stored in vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4ecea6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "When you load documents using a loader (like PyPDFLoader, TextLoader, etc.), each document is stored as a Document object in a list.\n",
    "\n",
    "Each Document object has two main parts:\n",
    "\n",
    "page_content → the actual text (main data).\n",
    "\n",
    "metadata → extra info (like page number, file name, source)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
